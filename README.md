You need to create an .env file with the api keys before running. To run the system, run ui.py; we are no longer using main.py. Also, you may need to add files in .venv including Lib, Scripts, and pycache

Technical description:
The general structure we used is the orchestrator-worker agentic AI workflow. Users can interact through either typed or spoken input as a WAV file via the UI. If a user speaks, the Speech-To-Text Agent using GPT-4.1-nano first transcribes the speech into text using OpenAI's Whisper-1 model before proceeding with processing. Multiple custom functions were needed to align this functionality with the UI. Once the input is in text form, the Coordinator Agent using GPT-4.1-nano analyzes it and routes it to one of three specialized agents based on the topic or context of the request. This agent also has a custom tool that it uses to print who it is routing the user input to in the console, as well as chat memory using OpenAI Sessions. For demo purposes, Sid (our course instructor) is a tech-savvy, humorous “son” persona, Jane Fonda is a nurturing “grandmother” figure who shares recipes and comforting advice, and LeBron James is a confident “uncle” persona who provides outdoor tips. All 3 of these worker-agents use OpenAI o4-mini for fast reasoning capabilities and all have access to the same chat history session as the Coordinator Agent and all have unique voices that were trained in ElevenLabs. Each has a custom tool that it can optionally use. Sid has an OpenAI Web Search tool, Jane has a TheMealDB Recipes (JSON) tool, and LeBron has an OpenWeather (JSON) tool, all via separate APIs. These agents simulate the personalities of people the user may have known in real life, creating a sense of familiarity and comfort. Once one of the worker-agents generates a response, it is passed to the Text-To-Speech Agent using GPT-4.1-mini. This agent uses 2 custom functions to retrieve the ElevenLabs Voice ID corresponding to the responding persona and get the final text that will be spoken aloud. Then, it uses a custom ElevenLabs API tool to generate natural-sounding audio in an mp3 file. The response is then played aloud to the user in the UI. All API keys were stored securely in a .env file.
